{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from huggingface_hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# Download Llama-3 model (replace with your Hugging Face token)\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `test` has been saved to /home/admin/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/admin/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"  # Replace with your token\n",
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/admin/slm-extreme-quantization/models/llama-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 16060.62 MB. The target location llama-3-8b/.cache/huggingface/download/original only has 7073.78 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 16060.62 MB. The target location llama-3-8b/original only has 7073.78 MB free disk space.\n",
      "  warnings.warn(\n",
      "Fetching 17 files:  65%|██████▍   | 11/17 [00:21<00:11,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create directories for models\n",
    "!mkdir -p models/llama-3\n",
    "%cd models/llama-3\n",
    "\n",
    "# Download Llama-3 model (8B version as example)\n",
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"meta-llama/Llama-3.1-8B\", local_dir=\"llama-3-8b\")\n",
    "\n",
    "# Return to main directory\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t  deepcompressor  exec.sh  models     run.sh\n",
      "build.sh  Dockerfile\t  mean.sh  readme.md  w4a16-quant.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: ./deepcompressor/: Is a directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/admin/slm-extreme-quantization/deepcompressor/deepcompressor/app/llm/ptq.py\", line 377, in <module>\n",
      "    config, _, unused_cfgs, unused_args, unknown_args = LlmPtqRunConfig.get_parser().parse_known_args()\n",
      "                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages/omniconfig/parser.py\", line 271, in parse_known_args\n",
      "    return self._parse_args(args=args, namespace=namespace, **defaults)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/admin/anaconda3/envs/deepcomp/lib/python3.12/site-packages/omniconfig/parser.py\", line 159, in _parse_args\n",
      "    assert os.path.isfile(path), f\"{path} is not a file\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: /home/admin/slm-extreme-quantization/examples/llm/configs/qoq-gchn.yaml is not a file\n"
     ]
    }
   ],
   "source": [
    "# Run QoQ quantization (W4A8KV4)\n",
    "! ./deepcompressor/\n",
    "!python -m deepcompressor.app.llm.ptq \\\n",
    "    /home/admin/slm-extreme-quantization/deepcompressor/examples/llm/configs/qoq-gchn.yaml \\\n",
    "    --model-name llama-3-8b \\\n",
    "    --model-path /home/admin/slm-extreme-quantization/models/llama-3/llama-3-8b/ \\\n",
    "    --smooth-proj-alpha 0 \\\n",
    "    --smooth-proj-beta 1 \\\n",
    "    --smooth-attn-alpha 0.5 \\\n",
    "    --smooth-attn-beta 0 \\\n",
    "    --save-model true \\\n",
    "    --eval-tasks wikitext \\\n",
    "    --eval-evaluators lm_eval\n",
    "# Convert to QServe format for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /home/admin/slm-extreme-quantization/examples/llm/configs/qoq-gchn.yaml: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat  /home/admin/slm-extreme-quantization/examples/llm/configs/qoq-gchn.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thon -m deepcompressor.backend.qserve.convert \\\n",
    "    --model-path models/llama-3/llama-3-8b \\\n",
    "    --quant-path ./quantized_model \\\n",
    "    --weight-bits 4 \\\n",
    "    --group-size 128 \\\n",
    "    --output-root ./qserve_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
