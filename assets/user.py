import random

from tqdm import tqdm
import torch

from lm_eval.api.model import LM
from lm_eval.api.registry import register_model
from lm_eval.api.instance import Instance
from lm_eval.utils import simple_parse_args_string


@register_model("user")
class UserLM(LM):
    # Load the model built with build.sh --compress
    def __init__(self, args ) -> None:
        super().__init__()
        argss = simple_parse_args_string(args)
        pretrained_path = argss['pretrained']
        self._model=torch.load(pretrained_path+"/mod.pt")

    @classmethod
    def create_from_arg_string(cls, arg_string, additional_config=None):
        return cls(arg_string)
    #https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/model_guide.md
    def loglikelihood(self, requests: list[Instance]) -> list[tuple[float, bool]]:
        """
        Args:
            requests: Each request contains Instance.args : Tuple[str, str] containing:
                1. an input string to the LM and
                2. a target string on which the loglikelihood of the LM producing this target,
                   conditioned on the input, will be returned.
        Returns:
            tuple (loglikelihood, is_greedy) for each request according to the input order:
                loglikelihood: probability of generating the target string conditioned on the input
                is_greedy: True if and only if the target string would be generated by greedy sampling from the LM
        """
        res = []
        for _ in tqdm(requests, disable=False):
            res.append((-random.random(), False))

        return res

    def generate_until(self, requests, disable_tqdm: bool = False):
        raise NotImplementedError()

    def loglikelihood_rolling(self, requests, disable_tqdm: bool = False):
        raise NotImplementedError()
